<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mathematica | adereth]]></title>
  <link href="http://adereth.github.io/blog/categories/mathematica/atom.xml" rel="self"/>
  <link href="http://adereth.github.io/"/>
  <updated>2014-01-06T07:22:50-05:00</updated>
  <id>http://adereth.github.io/</id>
  <author>
    <name><![CDATA[Matt Adereth]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking Mathematica on the Raspberry Pi]]></title>
    <link href="http://adereth.github.io/blog/2014/01/06/benchmarking-mathematica-on-the-raspberry-pi/"/>
    <updated>2014-01-06T07:23:00-05:00</updated>
    <id>http://adereth.github.io/blog/2014/01/06/benchmarking-mathematica-on-the-raspberry-pi</id>
    <content type="html"><![CDATA[<p>I&rsquo;m really excited about <a href="http://blog.wolfram.com/2013/11/21/putting-the-wolfram-language-and-mathematica-on-every-raspberry-pi/">Wolfram Research&rsquo;s announcement</a> that Mathematica and the Wolfram language are now available for free on the Raspberry Pi.</p>

<p>In the announcement, Stephen Wolfram gave this disclaimer:</p>

<blockquote><p>To be clear, the Raspberry Pi is perhaps 10 to 20 times slower at running the Wolfram Language than a typical current-model laptop (and sometimes even slower when itâ€™s lacking architecture-specific internal libraries).</p></blockquote>

<p>I&rsquo;ve got a laptop and a Raspberry Pi, so I decided to put this to the test.</p>

<h2>MathematicaMark</h2>

<p>Mathematica ships with <a href="http://reference.wolfram.com/mathematica/Benchmarking/tutorial/Benchmark.html">a benchmarking package called MathematicaMark</a>.  The latest version of the benchmark, <em>MathematicaMark9</em>, consists of 15 tests that use both numeric and symbolic functions.  The MathematicaMark score is the <a href="http://en.wikipedia.org/wiki/Geometric_mean">geometric mean</a> of the reciprocal of the timings, normalized against some reference system&rsquo;s timings:</p>

<p>$$ \sqrt[15]{\prod_{i=1}^{15} \frac{t_{r,i}}{t_{s,i}}} $$</p>

<p>&hellip;where $t_{s,i}$ is the timing for test $i$ on system $s$ and $r$ is the reference system.  For MathematicaMark9, the reference system is a 3.07 GHz Core i7-950 with 8 hyper-threaded cores running 64-bit Windows 7 Pro.  By definition, this system has a MathematicaMark9 score of 1.0.</p>

<p>We can compare systems using the MathematicaMark score.  If a system were 10 to 20 times slower, we would expect its MathematicaMark score to be anywhere from 1/10<sup>th</sup> to 1/20<sup>th</sup> the value of the faster system.  The <a href="http://reference.wolfram.com/mathematica/Benchmarking/ref/Benchmark.html"><code>Benchmark[]</code> function</a> also provides the timings for the individual tests, so we can dig in and see which functions might be benefitting from the architecture-specific internal libraries Wolfram mentioned.</p>

<h2>Raspberry Pi Configuration</h2>

<p><img src="/images/rpi.png" width="350"></p>

<p>I used a <a href="http://en.wikipedia.org/wiki/Raspberry_Pi#Specifications">Model B Raspberry Pi with 512MB of RAM</a>.  The tests were done after a fresh install of <a href="http://www.raspberrypi.org/archives/5580">NOOBS 1.3.3</a>, which includes Mathematica and the Wolfram Language installed by default.  <code>wolfram</code> was invoked from the commandline and nothing else was running on the system, most notably the X Window System and the <a href="http://reference.wolfram.com/mathematica/tutorial/UsingANotebookInterface.html">Mathematica Notebook interface</a>.</p>

<h2>&ldquo;Typical Current-Model Laptop&rdquo;</h2>

<p><img src="/images/mbp13.png" width="350"></p>

<p>Mathematica ships with benchmark results for 15 different systems (including the reference system).  It&rsquo;s not clear which system to use for this comparison, so I conveniently chose my Early 2013 13-inch Retina MacBook Pro, which sports a 2.6 GHz Intel Core i5 processor (4 hyper-threaded cores) as a representative &ldquo;typical current-model laptop.&rdquo;  Based on the sea of glowing Apple logos I&rsquo;ve seen in the audiences of the conferences I attended this year, I think it&rsquo;s a fair selection.</p>

<h2>MathematicaMark9 Scores</h2>

<p>With the setup out of the way, let&rsquo;s take a look at the report comparing the MacBook, Raspberry Pi, and the 15 included systems:</p>

<p><a href="/oneoff/mathematicamark9-20131231/"><img src="/images/MathematicaMark9.png" alt="MathematicaMark9 System Comparison Chart" /></a>
<em>Click for full-sized report</em></p>

<p>The MacBook Pro weighs in at a respectible 0.86, while the Raspberry Pi is actually getting rounded up to 0.01 from a true score of 0.005.  Running the benchmark takes 16 seconds on the laptop and <em>nearly 49 minutes</em> on the Raspberry Pi.</p>

<p>Even the slowest machine in the included benchmarks score nearly 30x higher.  I don&rsquo;t think Wolfram would consider a pre-Intel Mac to be a &ldquo;typical current-model&rdquo; computer.  To see the numbers he&rsquo;s citing, we need to dig into the timings for the individual tests.</p>

<h2>Performance on Individual Tests</h2>

<p>The source for the 15 individual tests and the timings on a variety of reference systems is included in the <a href="/oneoff/mathematicamark9-20131231/#sources">full MathematicaMark9 Benchmark Report</a>.  Here are the timings on the Raspberry Pi and the Macbook Pro:</p>

<table>
<thead>
<tr>
<th>Test </th>
<th align="right"> Pi Timing (s) </th>
<th align="right"> Mac Timing (s) </th>
<th align="right"> Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Number Sort </td>
<td align="right"> 25.13 </td>
<td align="right"> 1.75 </td>
<td align="right"> 14.4</td>
</tr>
<tr>
<td>Digits of Pi </td>
<td align="right"> 12.30 </td>
<td align="right"> 0.78 </td>
<td align="right"> 15.9</td>
</tr>
<tr>
<td>Matrix Arithmetic </td>
<td align="right"> 27.76 </td>
<td align="right"> 1.25 </td>
<td align="right"> 22.2</td>
</tr>
<tr>
<td>Gamma Function </td>
<td align="right"> 15.77 </td>
<td align="right"> 0.63 </td>
<td align="right"> 25.2</td>
</tr>
<tr>
<td>Large Integer Multiplication </td>
<td align="right"> 19.20 </td>
<td align="right"> 0.58 </td>
<td align="right"> 32.9</td>
</tr>
<tr>
<td>Polynomial Expansion </td>
<td align="right"> 4.55 </td>
<td align="right"> 0.12 </td>
<td align="right"> 36.4</td>
</tr>
<tr>
<td>Numerical Integration </td>
<td align="right"> 35.41 </td>
<td align="right"> 0.96 </td>
<td align="right"> 36.7</td>
</tr>
<tr>
<td>Matrix Transpose </td>
<td align="right"> 36.77 </td>
<td align="right"> 0.95 </td>
<td align="right"> 38.8</td>
</tr>
<tr>
<td>Data Fitting </td>
<td align="right"> 29.94 </td>
<td align="right"> 0.66 </td>
<td align="right"> 45.4</td>
</tr>
<tr>
<td>Discrete Fourier Transform </td>
<td align="right"> 79.28 </td>
<td align="right"> 0.95 </td>
<td align="right"> 83.4</td>
</tr>
<tr>
<td>Elementary Functions </td>
<td align="right"> 174.93 </td>
<td align="right"> 1.31 </td>
<td align="right"> 133.3</td>
</tr>
<tr>
<td>Eigenvalues of a Matrix </td>
<td align="right"> 136.87 </td>
<td align="right"> 0.79 </td>
<td align="right"> 174.1</td>
</tr>
<tr>
<td>Singular Value Decomposition </td>
<td align="right"> 433.08 </td>
<td align="right"> 1.52 </td>
<td align="right"> 284.0</td>
</tr>
<tr>
<td>Solving a Linear System </td>
<td align="right"> 745.53 </td>
<td align="right"> 1.65 </td>
<td align="right"> 452.1</td>
</tr>
<tr>
<td>Matrix Multiplication </td>
<td align="right"> 1136.51 </td>
<td align="right"> 2.15 </td>
<td align="right"> 528.9</td>
</tr>
</tbody>
</table>


<br/>


<p>Sorting by the ratio reveals that there are definitely cases where the relative performance falls in the 10x &ndash; 20x range cited by Wolfram.</p>

<p>It&rsquo;s interesting to note that the 4 worst performing tests by ratio are all linear algebra operations involving matrix decomposition or multiplication.  These are the types of operations that have probably gotten a lot of optimization love from Wolfram Research developers in the past because this is the area that potential users compare when deciding between Mathematica and its competitors, particularly Matlab.</p>

<p>If you look through <a href="http://www.wolfram.com/mathematica/quick-revision-history.html">the revision history highlights of Mathematica</a>, you&rsquo;ll see that there was a sequence of releases where every version had at least one top-level mention of linear algebra performance improvements:</p>

<ul>
<li>Mathematica 5.0 &ndash; 2003

<ul>
<li>&ldquo;Record-breaking speed through processor-optimized numerical linear algebra&rdquo;</li>
<li>&ldquo;Full support for high-speed sparse linear algebra&rdquo;</li>
</ul>
</li>
<li>Mathematica 5.1 &ndash; 2004

<ul>
<li>&ldquo;Numerical linear algebra performance enhancements&rdquo;</li>
</ul>
</li>
<li>Mathematica 5.2 &ndash; 2005

<ul>
<li>&ldquo;Multithreaded numerical linear algebra&rdquo;</li>
<li>&ldquo;Vector-based performance enhancements&rdquo;</li>
</ul>
</li>
</ul>


<p>The 5<sup>th</sup> worst test by ratio, Elementary Functions, is also interesting to dig into.  Here&rsquo;s the source:</p>

<p><code>clojure
Module[{m1, m2},
 Timing[
  SeedRandom[1];
  m1 = RandomReal[{}, {2.2`*^6}];
  m2 = RandomReal[{}, {2.2`*^6}];
  Do[
   Exp[m1];
   Sin[m1];
   ArcTan[m1, m2],
   {30}]]]
</code></p>

<p>It&rsquo;s computing $ e^x $, $ \sin{x} $, and $ \text{tan}^{-1} \frac{x}{y} $ for lists of 2,200,000 random numbers 30 times.  <code>Exp</code>, <code>Sin</code>, and <code>ArcTan</code> all have the <a href="http://reference.wolfram.com/mathematica/ref/Listable.html"><code>Listable</code> attribute</a>, which means that they are automatically mapped over lists that are passed in as arguments.  <code>Sin[list]</code> and <code>Map[Sin, list]</code> are functionally equivalent, but the former provides the implementation the opportunity to take an optimized path if there is a faster way of computing the sine of multiple numbers.</p>

<p>We can verify that this is a case where architecture specific optimizations are in play by rewriting the test to use <code>Map</code> and <code>MapThread</code>:</p>

<p><code>clojure
Module[{m1, m2},
 Timing[
  SeedRandom[1];
  m1 = RandomReal[{}, {2.2`*^6}];
  m2 = RandomReal[{}, {2.2`*^6}];
  Map[Exp, m1];
  Map[Sin, m1];
  MapThread[ArcTan, {m1, m2}];]]
</code></p>

<p>Note that I&rsquo;m only running this once, as opposed to the 30 times in the original test, because the non-Listable version is so much slower.</p>

<p>The version that doesn&rsquo;t take advantage of the Listable attribute takes 1.63 seconds on the Macbook Pro and 62.64 seconds on the Raspberry Pi.  This ratio of 38.2 (vs. 133.3 before) is much closer to the ratio we see from the other tests that don&rsquo;t take advantage of specifics of the architecture.</p>

<h2>Conclusion</h2>

<p>Even though Mathematica is much slower on the Raspberry Pi, it&rsquo;s a tremendous free gift and it still has many uses:</p>

<ul>
<li><p><a href="http://www.raspberrypi.org/archives/5623">A recent guest post from Wolfram Research on the Raspberry Pi blog</a> links to several projects that take advantage of the easy ways of controlling hardware using Mathematica on the Raspberry Pi.</p></li>
<li><p>Much of what most people use Mathematica for doesn&rsquo;t require extreme performance.  For instance, getting the closed form of an integral or derivative is still practically instantaneous from a human&rsquo;s perspective.</p></li>
<li><p>Just getting to experience the language and environment with only a $35 investment is worthwhile.  For developers, there is a lot to learn from the language, which is heavily influenced by <a href="http://en.wikipedia.org/wiki/M-expression">Lisp&rsquo;s M-expressions</a>, and the notebook enviroment, which is just starting to be replicated by iPython.  On top of that, the incredible interactive documentation for the language is something everyone should experience.</p></li>
</ul>


<p>Any questions, corrections, or suggestions are appreciated!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Iterating on Font Pair Comparisons]]></title>
    <link href="http://adereth.github.io/blog/2011/01/31/Iterating-on-Font-Pair-Comparisons/"/>
    <updated>2011-01-31T20:58:00-05:00</updated>
    <id>http://adereth.github.io/blog/2011/01/31/Iterating-on-Font-Pair-Comparisons</id>
    <content type="html"><![CDATA[<p>The previous post generated some great suggestions in the <a href="http://news.ycombinator.com/item?id=2147834">Hacker News discussion</a>.  I&rsquo;ve incorporated some of the feedback and hereâ€™s a summary of the changes:</p>

<ul>
<li>Changed the colors to improve contrast</li>
<li>Fixed the issue with the operator symbols (Thanks MMA_Pope!)</li>
<li>Increased the size of the characters and reduced the horizontal spacing</li>
<li>Added additional fonts</li>
</ul>


<p>Hereâ€™s the current list of fonts included in the comparison:</p>

<p><img src="/images/fontlist2.png" alt="Full font list" /></p>

<p>Take a look at the new version:</p>

<p><a href="/images/dejavusansmonodroidsansmono1.png"><img src="/images/dejavusansmonodroidsansmono1.png" alt="dejavusansmonodroidsansmono1.png" /></a>
(Click for full size version)</p>

<p>If youâ€™re interested in seeing the results for every pair, you can <a href="https://s3.amazonaws.com/1overBlog/programming_fonts/AllFontComparisonsV2.zip">download them all</a>.</p>

<p>The updated Mathematica code is available for <a href="https://s3.amazonaws.com/1overBlog/programming_fonts/PairCompareV2.nb">download</a>.</p>

<p>Iâ€™m open to suggestions for future investigationsâ€¦ thanks!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Comparing Programming Font Pairs]]></title>
    <link href="http://adereth.github.io/blog/2011/01/26/Comparing-Programming-Font-Pairs/"/>
    <updated>2011-01-26T22:46:00-05:00</updated>
    <id>http://adereth.github.io/blog/2011/01/26/Comparing-Programming-Font-Pairs</id>
    <content type="html"><![CDATA[<p>I really like this visualization of the difference between Deja Vu Sans Mono and Apple Menlo:</p>

<p><img src="http://typophile.com/files/menlovsdejavusansmono_6131.png" alt="Menlo vs. Deja Vu Sans Mono" /></p>

<p>(Credit to <a href="http://www.down10.com">Jesse Burgheimer</a>)</p>

<p>I decided to take a stab at programmatically generating a similar comparison for all the fonts Iâ€™ve been looking at.  <a href="/assets/paircompare.nb">Hereâ€™s the Mathematica code</a>.</p>

<p><a href="images/dejavusansmonodroidsansmono.png"><img src="images/dejavusansmonodroidsansmono.png" alt="Menlo vs. Deja Vu Sans Mono in Mathematica" /></a>
(Click for full size version)</p>

<p>It was then easy to generate this for every pair of fonts:</p>

<p><img src="/images/MapCompareFontPair.png" alt="CompareFontPair /@ Subsets[monospacedFonts, {2}]" /></p>

<p>I went through them all and picked out the ones I thought were most interesting:</p>

<p><a href="/images/consolasinconsolata.png"><img src="/images/consolasinconsolata.png" alt="consolasinconsolata" /></a>
<a href="/images/dejavusansmonodroidsansmono.png"><img src="/images/dejavusansmonodroidsansmono.png" alt="dejavusansmonodroidsansmono" /></a>
<a href="/images/dejavusansmonopanicsans.png"><img src="/images/dejavusansmonopanicsans.png" alt="dejavusansmonopanicsans" /></a>
<a href="/images/droidsansmonoinconsolata.png"><img src="/images/droidsansmonoinconsolata.png" alt="droidsansmonoinconsolata" /></a>
<a href="/images/envycoderterminus.png"><img src="/images/envycoderterminus.png" alt="envycoderterminus" /></a>
<a href="/images/inconsolatamonofur.png"><img src="/images/inconsolatamonofur.png" alt="inconsolatamonofur" /></a></p>

<p>If youâ€™re interested in seeing the results for every pair, you can <a href="https://s3.amazonaws.com/1overBlog/programming_fonts/AllFontPairComparisons.zip">download them all</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Comparing Troublesome Groups]]></title>
    <link href="http://adereth.github.io/blog/2011/01/26/Comparing-Troublesome-Groups/"/>
    <updated>2011-01-26T10:47:00-05:00</updated>
    <id>http://adereth.github.io/blog/2011/01/26/Comparing-Troublesome-Groups</id>
    <content type="html"><![CDATA[<p>One of the comments on the <a href="http://www.reddit.com/r/programming/comments/f8nzc/programmatic_programming_font_comparison_101_vs/">reddit discussion</a> of my first post suggested looking at groups of characters that might be troublesome:
<code>
1l|i!
`',;:.
oO0Q
(){}[]
5S
</code>
As a quick test, I wrote something to compare every pair of characters in a list:</p>

<p><img src="/images/tallskinnycode.png" alt="Mathematica code for comparing groups" /></p>

<p>Click to see the full-sized result:</p>

<p><a href="/images/tallskinnycomparison.png"><img src="/images/tallskinnycomparison.png" alt="Comparison results" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[101 vs. lol - More Fonts]]></title>
    <link href="http://adereth.github.io/blog/2011/01/25/101-vs-lol%E2%80%93More-Fonts/"/>
    <updated>2011-01-25T12:07:00-05:00</updated>
    <id>http://adereth.github.io/blog/2011/01/25/101-vs-lolâ€“More-Fonts</id>
    <content type="html"><![CDATA[<p>As requested in the <a href="http://www.reddit.com/r/programming/comments/f8nzc/programmatic_programming_font_comparison_101_vs/">reddit discussion</a>, here are Envy Code R, Liberation Mono, and Terminus:</p>

<p><img src="/images/envy-code-r.png" alt="Envy Code R" />
<img src="/images/liberation-mono.png" alt="Liberation Mono" />
<img src="/images/terminus.png" alt="Terminus" /></p>
]]></content>
  </entry>
  
</feed>
