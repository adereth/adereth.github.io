<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: algorithms | adereth]]></title>
  <link href="http://adereth.github.io/blog/categories/algorithms/atom.xml" rel="self"/>
  <link href="http://adereth.github.io/"/>
  <updated>2014-11-10T17:06:00-08:00</updated>
  <id>http://adereth.github.io/</id>
  <author>
    <name><![CDATA[Matt Adereth]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Silverman's Mode Estimation Method Explained]]></title>
    <link href="http://adereth.github.io/blog/2014/10/12/silvermans-mode-detection-method-explained/"/>
    <updated>2014-10-12T12:51:00-07:00</updated>
    <id>http://adereth.github.io/blog/2014/10/12/silvermans-mode-detection-method-explained</id>
    <content type="html"><![CDATA[<script src="http://d3js.org/d3.v2.js"></script>


<div>
  <style type="text/css">

     .chart {
       font-size: 10px;
       margin-top: -40px;
     }

     .point {
       fill: steelblue;
       fill-opacity: 0.5;
       stroke: black;
       stroke-width: 1
       stroke-opacity: 0.5;
     }

     .axis path, .axis line {
       fill: none;
       stroke: #000;
       shape-rendering: crispEdges;
     }

     .area {
       fill: steelblue;
       fill-opacity: 0.25;
       stroke: #000;
       stroke-opacity: 0.5;
     }

     .summedarea {
       fill: steelblue;
       fill-opacity: 0.75;
       stroke: #000;
       stroke-opacity: 0.5;
     }

    .bar rect {
        fill: steelblue;
        fill-opacity: 0.75;
        shape-rendering: crispEdges;
        stroke: #000;
        stroke-opacity: 0.5;

    }

    .bar text {
        fill: #fff;
    }


  </style>
</div>




<!-- Global Variables and Handlers: -->


<script type="text/javascript">

  var margin = {top: 50, right: 40, bottom: 40, left: 60},
      width = $('.entry-content').width();

  $(window).resize(function() {
    width = $('.entry-content').width();
  });

  function drawPoints(data, chart, height) {

    $(chart).empty();

    var x = d3.scale.linear()
        .domain([0, d3.max(data, function(d) { return d.value}) + 5])
        .range([0, width - margin.left - margin.right]);

    var y = d3.scale.ordinal()
        .domain(d3.range(data.length))
        .rangeRoundBands([height - margin.top - margin.bottom, 0], 0.2);

    var xAxis = d3.svg.axis()
        .scale(x)
        .orient('bottom')
        .tickPadding(8)
    .ticks(8);

    var yAxis = d3.svg.axis()
        .scale(y)
        .orient('left')
        .tickPadding(8)
        .tickSize(0);

    var svg = d3.select(chart).append('svg')
        .attr('width', width)
        .attr('height', height)
        .attr('class', 'chart')
          .append('g')
        .attr('transform', 'translate(' + margin.left + ', ' + margin.top + ')');

    svg.selectAll('.chart')
        .data(data)
        .enter().append('circle')
        .attr('class', 'point')
        .attr('cx', function(d, i) { return x(d.value) })
        .attr('cy', 0)
        .attr('r', 3);

    svg.append('g')
        .attr('class', 'x axis')
        .call(xAxis);

  }

  function drawPointsWithResize(data, chart, height) {
    drawPoints(data, chart, height);
    $(window).resize(function() {drawPoints(data, chart, height); })
  };


     function drawOverlappingDistributions(data, chart, height) {

       $(chart).empty();

       var x = d3.scale.linear()
                       .domain([0, d3.max(data, function(d) { return d.value}) + 5])
                       .range([0, width - margin.left - margin.right]);

       var y = d3.scale.linear()
                       .domain([0, 0.5])
                       .range([height - margin.top - margin.bottom, 0]);

       var xAxis = d3.svg.axis()
                         .scale(x)
                         .orient('bottom')
                         .tickPadding(8)
                         .ticks(8);

       var yAxis = d3.svg.axis()
                         .scale(y)
                         .orient('left')
                         .tickPadding(8)
                         .tickSize(0);

       var svg = d3.select(chart).append('svg')
                   .attr('width', width)
                   .attr('height', height)
                   .attr('class', 'chart')
                   .append('g')
                   .attr('transform', 'translate(' + margin.left + ', ' + margin.top + ')');

       var line = d3.svg.line()
                        .x(function(d) {return x(d.q)})
                        .y(function(d) {return y(d.p)})

       var scale = 1 / Math.sqrt(2 * Math.PI);
       function gaussian(x, mean, sigma) {
         var z = (x - mean) / sigma;
         return scale * Math.exp(-0.5 * z * z) / sigma;
       };

       function subpoints(d) {
         return d3.range(d.value - 4, d.value + 4, 0.1).map(
           function (d2,i,a) {
             return {value: d2, height: gaussian(d2, d.value, 1)};
           });
       }

       data.forEach(function(d) {

         var area = d3.svg.area()
                          .interpolate("monotone")
                          .x(function(d) { return x(d.value); })
                          .y0(y(0))
                          .y1(function(d) { return y(d.height); });

         svg.append('path')
            .attr('class', 'area')
            .attr("d", area(subpoints(d)))
       });

       svg.selectAll('.chart')
          .data(data)
          .enter().append('circle')
          .attr('class', 'point')
          .attr('cx', function(d, i) { return x(d.value) })
          .attr('cy', y(0))
          .attr('r', 3);

       svg.append('g')
          .attr('class', 'x axis')
          .attr("transform", "translate(0," + (height - margin.top - margin.bottom) + ")")
          .call(xAxis);

     }

     function drawOverlappingDistributionsWithResize(data, chart, height) {
       drawOverlappingDistributions(data, chart, height);
       $(window).resize(function() {drawOverlappingDistributions(data, chart, height); })
     };


     function drawSummedDistributions(data, chart, height, stddev) {

       $(chart).empty();

       var scale = 1 / Math.sqrt(2 * Math.PI);
       function gaussian(x, mean, sigma) {
         var z = (x - mean) / sigma;
         return scale * Math.exp(-0.5 * z * z) / sigma;
       };

       var points = d3.range(0, 30, 0.01).concat(data.map(function(x) {return x.value}))
       .sort(function(a,b){return a-b})
                      .map(
         function (x,i,a) {
           var y = 0;
           data.forEach(function(d) {
             y += gaussian(x, d.value, stddev)
           });
           return {value: x, height: y};
         }

       );



       var x = d3.scale.linear()
                       .domain([0, d3.max(data, function(d) { return d.value}) + 5])
                       .range([0, width - margin.left - margin.right]);

       var y = d3.scale.linear()
                       .domain([0, d3.max(points, function(d) { return d.height})])
                       .range([height - margin.top - margin.bottom, 0]);

       var xAxis = d3.svg.axis()
                         .scale(x)
                         .orient('bottom')
                         .tickPadding(8)
                         .ticks(8);

       var yAxis = d3.svg.axis()
                         .scale(y)
                         .orient('left')
                         .tickPadding(8)
                         .tickSize(0);

       var svg = d3.select(chart).append('svg')
                   .attr('width', width)
                   .attr('height', height)
                   .attr('class', 'chart')
                   .append('g')
                   .attr('transform', 'translate(' + margin.left + ', ' + margin.top + ')');

       var line = d3.svg.line()
                        .x(function(d) {return x(d.q)})
                        .y(function(d) {return y(d.p)})

       var area = d3.svg.area()
                        .interpolate("monotone")
                        .x(function(d) { return x(d.value); })
                        .y0(y(0))
                        .y1(function(d) { return y(d.height); });

       svg.append('path')
          .attr('class', 'summedarea')
          .attr("d", area(points))

       svg.selectAll('.chart')
          .data(data)
          .enter().append('circle')
          .attr('class', 'point')
          .attr('cx', function(d, i) { return x(d.value) })
          .attr('cy', y(0))
          .attr('r', 3);


       svg.append('g')
          .attr('class', 'x axis')
          .attr("transform", "translate(0," + (height - margin.top - margin.bottom) + ")")
          .call(xAxis);

     }



     function drawSummedDistributionsWithResize(data, chart, height, stddev) {
       drawSummedDistributions(data, chart, height, stddev);
       $(window).resize(function() {drawSummedDistributions(data, chart, height, stddev); })
     };

     function drawHistogram(data, chart, height) {

       $(chart).empty();

       var x = d3.scale.linear()
                       .domain([0, 11])
                       .range([0, width - margin.left - margin.right]);

       var data = d3.layout.histogram()
                           .bins(x.ticks(10))(data);

       var y = d3.scale.linear()
                       .domain([0, d3.max(data, function(d) { return d.y; })])
                       .range([height - margin.top - margin.bottom, 0]);

       var xAxis = d3.svg.axis()
                         .scale(x)
                         .orient('bottom')
                         .tickPadding(8)
                         .ticks(8);

       var yAxis = d3.svg.axis()
                         .scale(y)
                         .orient('left')
                         .tickPadding(8)
                         .tickSize(0);

       var svg = d3.select(chart).append('svg')
                   .attr('width', width)
                   .attr('height', height)
                   .attr('class', 'chart')
                   .append('g')
                   .attr('transform', 'translate(' + margin.left + ', ' + margin.top + ')');

       var bar = svg.selectAll(".bar")
                    .data(data)
                    .enter().append("g")
                    .attr("class", "bar")
                    .attr("transform", function(d) { return "translate(" + x(d.x - 0.5) + "," + y(d.y) + ")"; });

       bar.append("rect")
          .attr("width", x(data[0].dx) - 3)
          .attr("height", function(d) {
            console.log(d + " " + y(d.y));
            return height - y(d.y) - margin.bottom - margin.top; });

       svg.append("g")
          .attr("class", "x axis")
          .attr("transform", "translate(0," + (height - margin.top - margin.bottom) + ")")
          .call(xAxis);

       svg.append("g")
          .attr("class", "y axis")
          .call(yAxis);


     }

     function drawHistogramWithResize(data, chart, height) {
       drawHistogram(data, chart, height);
       $(window).resize(function() {drawHistogram(data, chart, height); })
     };


</script>


<p>I started digging into the history of mode detection after watching <a href="http://aysy.lu/">Aysylu Greenberg</a>&rsquo;s <a href="http://youtu.be/XmImGiVuJno">Strange Loop talk on benchmarking</a>.  She pointed out that the usual benchmarking statistics fail to capture that our timings may actually be samples from multiple distributions, commonly caused by the fact that our systems are comprised of hierarchical caches.</p>

<p>I thought it would be useful to add the detection of this to my favorite benchmarking tool, <a href="http://hugoduncan.org/">Hugo Duncan</a>&rsquo;s <a href="https://github.com/hugoduncan/criterium">Criterium</a>.  Not surprisingly, Hugo had already considered this and there&rsquo;s a note under the TODO section:</p>

<p><code>
Multimodal distribution detection.
Use kernel density estimators?
</code></p>

<p>I hadn&rsquo;t heard of using kernel density estimation for multimodal distribution detection so I found the original paper, <a href="http://www.stat.washington.edu/wxs/Stat593-s03/Literature/silverman-81a.pdf">Using Kernel Density Estimates to Investigate Multimodality (Silverman, 1981)</a>.  The original paper is a dense 3 pages and my goal with this post is to restate Silverman&rsquo;s method in a more accessible way.  Please excuse anything that seems overly obvious or pedantic and feel encouraged to suggest any modifications that would make it clearer.</p>

<h2>What is a mode?</h2>

<p>The mode of a distribution is the value that has the highest probability of being observed.  Many of us were first exposed to the concept of a mode in a discrete setting.  We have a bunch of observations and the mode is just the observation value that occurs most frequently.  It&rsquo;s an elementary exercise in counting.  Unfortunately, this method of counting doesn&rsquo;t transfer well to observations sampled from a continuous distribution because we don&rsquo;t expect to ever observe the exact some value twice.</p>

<p>What we&rsquo;re really doing when we count the observations in the discrete case is estimating the <a href="http://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (PDF) of the underlying distribution.  The value that has the highest probability of being observed is the one that is the global maximum of the PDF.  Looking at it this way, we can see that a necessary step for determining the mode in the continuous case is to first estimate the PDF of the underlying distribution.  We&rsquo;ll come back to how Silverman does this with a technique called kernel density estimation later.</p>

<h2>What does it mean to be multimodal?</h2>

<p>In the discrete case, we can see that there might be undeniable multiple modes because the counts for two elements might be the same.  For instance, if we observe:</p>

<p>$$1,2,2,2,3,4,4,4,5$$</p>

<p>Both 2 and 4 occur thrice, so we have no choice but to say they are both modes.  But perhaps we observe something like this:</p>

<p>$$1,1,1,2,2,2,2,3,3,3,4,9,10,10$$</p>

<p>The value 2 occurs more than anything else, so it&rsquo;s <em>the</em> mode.  But let&rsquo;s look at the histogram:</p>

<div id='hist'></div>


<script type='text/javascript'>
drawHistogramWithResize([1,1,1,2,2,2,2,3,3,3,4,9,10,10], '#hist', 300);
</script>


<p>That pair of 10&rsquo;s are out there looking awfully interesting.  If these were benchmark timings, we might suspect there&rsquo;s a significant fraction of calls that go down some different execution path or fall back to a slower level of the cache hierarchy.  Counting alone isn&rsquo;t going to reveal the 10&rsquo;s because there are even more 1&rsquo;s and 3&rsquo;s.  Since they&rsquo;re nestled up right next to the 2&rsquo;s, we probably will assume that they are just part of the expected variance in performance of the same path that caused all those 2&rsquo;s.  <em>What we&rsquo;re really interested in is the local maxima of the PDF because they are the ones that indicate that our underlying distribution may actually be a mixture of several distributions.</em></p>

<h2>Kernel density estimation</h2>

<p>Imagine that we make 20 observations and see that they are distributed like this:</p>

<div id='chart-1'></div>


<script type='text/javascript'>

  var data = [
{value: 13.1138}, {value: 10.6519}, {value: 20.5735}, {value: 7.89327}, {value: 9.02554}, {value: 20.8411}, {value: 8.84072}, {value: 10.6273}, {value: 13.5194}, {value: 17.9757}, {value: 10.1086}, {value: 8.68131}, {value: 7.16192}, {value: 19.9496}, {value: 8.77111}, {value: 19.5314}, {value: 9.40915}, {value: 12.8664}, {value: 23.1322}, {value: 13.5008}];
  drawPointsWithResize(data, '#chart-1', 90);
</script>


<p>We can estimate the underlying PDF by using what is called a <a href="http://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimate</a>.  We replace each observation with some distribution, called the &ldquo;kernel,&rdquo; centered at the point.  Here&rsquo;s what it would look like using a normal distribution with standard deviation 1:</p>

<div id='chart-2'></div>


<script type='text/javascript'>
    drawOverlappingDistributionsWithResize(data, '#chart-2', 200);
</script>


<p>If we sum up all these overlapping distributions, we get a reasonable estimate for the underlying continuous PDF:</p>

<div id='chart-3'></div>


<script type='text/javascript'>
     drawSummedDistributionsWithResize(data, '#chart-3', 300, 1);
</script>


<p>Note that we made two interesting assumptions here:</p>

<ol>
<li><p>We replaced each point with a normal distribution.  Silverman&rsquo;s approach actually relies on some of the nice mathematical properties of the normal distribution, so that&rsquo;s what we use.</p></li>
<li><p>We used a standard deviation of 1.  Each normal distribution is wholly specified by a mean and a standard deviation.  The mean is the observation we are replacing, but we had to pick some arbitrary standard deviation which defined the width of the kernel.</p></li>
</ol>


<p>In the case of the normal distribution, we could just vary the standard deviation to adjust the width, but there is a more general way of stretching the kernel for arbitrary distributions.  The kernel density estimate for observations $X_1,X_2,&hellip;,X_n$ using a kernel function $K$ is:</p>

<p>$$\hat{f}(x)=\frac{1}{n}\sum\limits_{i=1}^n K(x-X_i)$$</p>

<p>In our case above, $K$ is the PDF for the normal distribution with standard deviation 1.  We can stretch the kernel by a factor of $h$ like this:</p>

<p>$$\hat{f}(x, h)=\frac{1}{nh}\sum\limits_{i=1}^n K(\frac{x-X_i}{h})$$</p>

<p>Note that changing $h$ has the exact same effect as changing the standard deviation: it makes the kernel wider and shorter while maintaining an area of 1 under the curve.</p>

<h2>Different kernel widths result in different mode counts</h2>

<p>The width of the kernel is effectively a smoothing factor.  If we choose too large of a width, we just end up with one giant mound that is almost a perfect normal distribution.  Here&rsquo;s what it looks like if we use $h=5$:</p>

<div id='chart-4'></div>


<script type='text/javascript'>
     drawSummedDistributionsWithResize(data, '#chart-4', 300, 5);
</script>


<p>Clearly, this has a single maxima.</p>

<p>If we choose too small of a width, we get a very spiky and over-fit estimate of the PDF.  Here&rsquo;s what it looks like with $h = 0.1$:</p>

<div id='chart-5'></div>


<script type='text/javascript'>
drawSummedDistributionsWithResize(data, '#chart-5', 300, 0.1);
</script>


<p>This PDF has a bunch of local maxima.  If we shrink the width small enough, we&rsquo;ll get $n$ maxima, where $n$ is the number of observations:</p>

<div id='chart-6'></div>


<script type='text/javascript'>
drawSummedDistributionsWithResize(data, '#chart-6', 300, 0.005);
</script>


<p>The neat thing about using the normal distribution as our kernel is that it has the property that shrinking the width will only introduce new local maxima.  Silverman gives a proof of this at the end of Section 2 in the original paper.  This means that for every integer $k$, where $1&lt;k&lt;n$, we can find the minimum width $h_k$ such that the kernel density estimate has at most $k$ maxima.  Silverman calls these $h_k$ values &ldquo;critical widths.&rdquo;</p>

<h2>Finding the critical widths</h2>

<p>To actually find the critical widths, we need to look at the formula for the kernel density estimate.  The PDF for a plain old normal distribution with mean $\mu$ and standard deviation $\sigma$ is:</p>

<p>$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}\mathrm{e}^{&ndash;\frac{(x-\mu)^2}{2\sigma^2}}$$</p>

<p>The kernel density estimate with standard deviation $\sigma=1$ for observations $X_1,X_2,&hellip;,X_n$ and width $h$ is:</p>

<p>$$\hat{f}(x,h)=\frac{1}{nh}\sum\limits_{i=1}^n \frac{1}{\sqrt{2\pi}}\mathrm{e}^{&ndash;\frac{(x-X_i)^2}{2h^2}}$$</p>

<p>For a given $h$, you can find all the local maxima of $\hat{f}$ using your favorite numerical methods.  Now we need to find the $h_k$ where new local maxima are introduced.  Because of a result that Silverman proved at the end of section 2 in the paper, we know we can use a binary search over a range of $h$ values to find the critical widths at which new maxima show up.</p>

<h2>Picking which kernel width to use</h2>

<p>This is the part of the original paper that I found to be the least clear.  It&rsquo;s pretty dense and makes a number of vague references to the application of techniques from other papers.</p>

<p>We now have a kernel density estimate of the PDF for each number of modes between $1$ and $n$.  For each estimate, we&rsquo;re going to use a statistical test to determine the significance.  We want to be parsimonious in our claims that there are additional modes, so we pick the smallest $k$ such that the significance measure of $h_k$ meets some threshold.</p>

<p><a href="http://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a> is used to evaluate the accuracy of a statistical measure by computing that statistic on observations that are <a href="http://en.wikipedia.org/wiki/Resampling_(statistics)">resampled</a> from the original set of observations.</p>

<p>Silverman used a <a href="http://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Smooth_bootstrap">smoothed bootstrap procedure</a> to evaluate the significance.  Smoothed bootstrapping is bootstrapping with some noise added to the resampled observations.  First, we sample from the original set of observations, with replacement, to get $X_I(i)$.  Then we add noise to get our smoothed $y_i$ values:</p>

<p>$$y_i=\frac{1}{\sqrt{1+h_k^2/\sigma^2}}(X_{I(i)}+h_k \epsilon_i)$$</p>

<p>Where $\sigma$ is the standard deviation of $X_1,X_2,&hellip;,X_n$, $h_k$ is the critical width we are testing, and $\epsilon_i$ is a random value sampled from a normal distribution with mean 0 and standard deviation 1.</p>

<p>Once we have these smoothed values, we compute the kernel density estimate of them using $h_k$ and count the modes.  If this kernel density estimate doesn&rsquo;t have more than $k$ modes, we take that as a sign that we have a good critical width.  We repeat this many times and use the fraction of simulations where we didn&rsquo;t find more than $k$ modes as the p-value.  In the paper, Silverman does 100 rounds of simulation.</p>

<h2>Conclusion</h2>

<p>Silverman&rsquo;s technique was a really important early step in multimodality detection and it has been thoroughly investigated and improved upon since 1981.  Google Scholar lists <a href="http://scholar.google.com/scholar?espv=2&amp;bav=on.2,or.r_cp.r_qf.&amp;bvm=bv.77161500,d.cGE&amp;ion=1&amp;biw=1680&amp;bih=938&amp;dpr=2&amp;um=1&amp;ie=UTF-8&amp;lr=&amp;cites=18163244822709704741">about 670 citations of this paper</a>.  If you&rsquo;re interested in learning more, one paper I found particularly helpful was <a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/A11n28.pdf">On the Calibration of Silverman&rsquo;s Test for Multimodality (Hall &amp; York, 2001)</a>.</p>

<p>One of the biggest weaknesses in Silverman&rsquo;s technique is that the critical width is a global parameter, so it may run into trouble if our underlying distribution is a mixture of low and high variance component distributions.  For an actual implementation of mode detection in a benchmarking package, I&rsquo;d consider using something that doesn&rsquo;t have this issue, like the technique described in <a href="http://private.igf.edu.pl/~jnn/Literatura_tematu/Minnotte_1997.pdf">Nonparametric Testing of the Existence of Modes (Minnotte, 1997)</a>.</p>

<p>I hope this is correct and helpful.  If I misinterpreted anything in the original paper, please let me know.  Thanks!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Computing the Remedian in Clojure]]></title>
    <link href="http://adereth.github.io/blog/2014/09/29/computing-the-remedian-in-clojure/"/>
    <updated>2014-09-29T09:03:00-07:00</updated>
    <id>http://adereth.github.io/blog/2014/09/29/computing-the-remedian-in-clojure</id>
    <content type="html"><![CDATA[<p>The remedian is an approximation of the <a href="http://en.wikipedia.org/wiki/Median">median</a> that can be computed using only $O(\log{n})$ storage.  The algorithm was originally presented in <a href="http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/Remedian.pdf">The Remedian: A Robust Averaging Method for Large Data Sets by Rousseeuw and Bassett</a> (1990).  The core of it is on the first page:</p>

<blockquote><p>Let us assume that $n = b^k$, where $b$ and $k$ are integers (the case where $n$ is not of this form will be treated in Sec. 7.  The <em>remedian</em> with base $b$ proceeds by computing medians of groups of $b$ observations, yielding $b^{k-1}$ estimates on which this procedure is iterated, and so on, until only a single estimate remains.  When implemented properly, this method merely needs $k$ arrays of size $b$ that are continuously reused.</p></blockquote>

<p>The implementation of this part in Clojure is so nice that I just had to share.</p>

<p>First, we need a vanilla implementation of the median function.  We&rsquo;re always going to be computing the median of sets of size $b$, where $b$ is relatively small, so there&rsquo;s no need to get fancy with a linear time algorithm.</p>

<p>```clojure
(defn median [coll]
  (let [size (count coll)</p>

<pre><code>    sorted (sort coll)]
(if (odd? size)
  (nth sorted (int (/ size 2)))
  (/ (+ (nth sorted (int (/ size 2)))
        (nth sorted (dec (int (/ size 2)))))
     2))))
</code></pre>

<p>```</p>

<p>Now we can implement the actual algorithm.  We group, compute the median of each group, and recur, with the base case being when we&rsquo;re left with a single element in the collection:</p>

<p>```clojure
(defn remedian [b coll]
  (if (next coll)</p>

<pre><code>(-&gt;&gt; coll
     (partition-all b)
     (map median)
     (recur b))
(first coll)))
</code></pre>

<p>```</p>

<p>Because <code>partition-all</code> and <code>map</code> both operate on and return lazy sequences, we maintain the property of only using $O(b \log_{b}{n})$ memory at any point in time.</p>

<p>While this implementation is simple and elegant, it only works if the size of the collection is a power of $b$.  If we don&rsquo;t have $n = b^k$ where $b$ and $k$ are integers, we&rsquo;ll over-weight the observations that get grouped into the last groups of size $&lt; b$.</p>

<p>Section 7 of the original paper describes the weighting scheme you should use to compute the median if you&rsquo;re left with incomplete groupings:</p>

<blockquote><p>How should we proceed when the sample size $n$ is less than $b^k$? The remedian algorithm then ends up with $n_1$ numbers in the first array, $n_2$ numbers in the second array, and $n_k$ numbers in the last array, such that $n = n_1 + n_{2}b + &hellip; + n_k b^{k-1}$.  For our final estimate we then compute a weighted median in which the $n_1$, numbers in the first array have weight 1, the $n_2$ numbers in the second array have weight $b$, and the $n_k$ numbers in the last array have weight $b^{k-1}$. This final computation does not need much storage because there are fewer than $bk$ numbers and they only have to be ranked in increasing order, after which their weights must be added until the sum is at least $n/2$.</p></blockquote>

<p>It&rsquo;s a bit difficult to directly translate this to the recursive solution I gave above because in the final step we&rsquo;re going to do a computation on a mixture of values from the different recursive sequences.  Let&rsquo;s give it a shot.</p>

<p>We need some way of bubbling up the incomplete groups for the final weighted median computation.  Instead of having each recursive sequence <em>always</em> compute the median of each group, we can add a check to see if the group is smaller than $b$ and, if so, just return the incomplete group:</p>

<p>```clojure
(defn remedian-with-leftovers [b coll]
  (let [incomplete-group? #(or (&lt; (count %) b)</p>

<pre><code>                           (seq? (last %)))]
(loop [coll coll]
  (if (next coll)
    (-&gt;&gt; coll
         (partition-all b)
         (map #(if (incomplete-group? %) % (median %)))
         (recur))
    coll))))
</code></pre>

<p>```</p>

<p>For example, if we were using the mutable array implementation proposed in the original paper to compute the remedian of <code>(range 26)</code> with $b = 3$, the final state of the arrays would be:</p>

<table>
<thead>
<tr>
<th>Array  </th>
<th> $i_0$ </th>
<th> $i_1$   </th>
<th> $i_2$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0      </td>
<td> 24 </td>
<td> 25 </td>
<td> <em>empty</em></td>
</tr>
<tr>
<td>1      </td>
<td> 19 </td>
<td> 22 </td>
<td> <em>empty</em></td>
</tr>
<tr>
<td>2      </td>
<td> 4  </td>
<td> 13 </td>
<td> <em>empty</em></td>
</tr>
</tbody>
</table>


<br/>


<p>In our sequence based solution, the final sequence will be <code>((4 13 (19 22 (24 25))))</code>.</p>

<p>Now, we need to convert these nested sequences into <code>[value weight]</code> pairs that could be fed into a weighted median function:</p>

<p>```clojure
(defn weight-leftovers [b nested-elements]
  (loop [vw-pairs []</p>

<pre><code>     nested-elements nested-elements
     weight 1]
(let [element (first nested-elements)]
  (cond
   (next nested-elements) (recur (conj vw-pairs [element weight])
                                 (next nested-elements)
                                 weight)
   (seq? element) (recur vw-pairs
                         element
                         (/ weight b))
   :else (conj vw-pairs [element weight])))))
</code></pre>

<p>```
Instead of weighting the values in array $j$ with weight $b^{j-1}$, we&rsquo;re weighting it at $\frac{b^{j-1}}{b^{k}}$.  Dividing all the weights by a constant will give us the same result and this is slightly easier to compute recursively, as we can just start at 1 and divide by $b$ as we descend into each nested sequence.</p>

<p>If we run this on the <code>(range 26)</code> with $b = 3$, we get:</p>

<p>```clojure
user> (&ndash;>> (range 26)</p>

<pre><code>       (remedian-with-leftovers 3)
       (weight-leftovers 3))
</code></pre>

<p>[[4 1/3] [13 1/3] [19 1/9] [22 1/9] [24 1/27] [25 1/27]]
```</p>

<p>Finally, we&rsquo;re going to need a weighted median function.  This operates on a collection of <code>[value weight]</code> pairs:</p>

<p>```clojure
(defn weighted-median [vw-pairs]
  (let [total-weight (&ndash;>> vw-pairs</p>

<pre><code>                      (map second)
                      (reduce +))
    middle-weight (/ total-weight 2)
    sorted-pairs (sort-by first vw-pairs)
    sorted-pairs-cum-weight (reductions (fn [[_ cum-weight] [v w]]
                                          [v (+ cum-weight w)])
                                        sorted-pairs)]
(-&gt;&gt; sorted-pairs-cum-weight
     (filter #(&lt;= middle-weight (second %)))
     (ffirst))))
</code></pre>

<p>```</p>

<p>We can put it all together and redefine the remedian function to deal with the case where $n$ isn&rsquo;t a power of $b$:</p>

<p>```clojure
(defn remedian [b coll]
  (&ndash;>> coll</p>

<pre><code>   (remedian-with-leftovers b)
   (weight-leftovers b)
   (weighted-median)))
</code></pre>

<p>```</p>

<p>The remedian is fun, but in practice I prefer to use the approximate quantile methods that were invented a few years later and presented in <a href="http://www.cs.umd.edu/~samir/498/manku.pdf">Approximate Medians and other Quantiles in One Pass and with Limited Memory by Manku, Rajagopalan, and Lindsay</a> (1998).  There&rsquo;s a high-quality implementation you can use in Clojure via Java interop in Parallel Colt&rsquo;s <a href="http://incanter.org/docs/parallelcolt/api/cern/jet/stat/tdouble/quantile/DoubleQuantileFinderFactory.html">DoubleQuantileFinderFactory</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Efficiently Computing Kendall's Tau]]></title>
    <link href="http://adereth.github.io/blog/2013/10/30/efficiently-computing-kendalls-tau/"/>
    <updated>2013-10-30T21:45:00-07:00</updated>
    <id>http://adereth.github.io/blog/2013/10/30/efficiently-computing-kendalls-tau</id>
    <content type="html"><![CDATA[<p>Typically when people talk about correlation they are referring to the <a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson&rsquo;s product-moment coefficient</a>:</p>

<p>$$\rho_{X,Y}={E[(X-\mu_X)(Y-\mu_Y)] \over \sigma_X\sigma_Y}$$</p>

<p>The Pearson coefficient is 1 if the datasets have a perfectly positive linear relationship and -1 if they have a perfectly negative linear relationship.  But what if our data has a clear positive relationship, but it&rsquo;s not linear?  Or what if our data isn&rsquo;t even numeric and doesn&rsquo;t have a meaningful way of computing the average, $\mu$, or standard deviation, $\sigma$?</p>

<p>In these cases, Kendall&rsquo;s Tau is a useful way of measuring the correlation since it only requires that we have a <a href="http://en.wikipedia.org/wiki/Total_order">total ordering</a> for each of our datasets.  For each pair of observations, $(x_1, y_1)$ and $(x_2, y_2)$, we call the pair <em>concordant</em> if:
$$x_1 &lt; x_2 \text{ and } y_1 &lt; y_2$$
$$\text{or}$$
$$x_1 > x_2 \text{ and } y_1 > y_2$$
&hellip;and we call the pair <em>discordant</em> if:
$$x_1 &lt; x_2 \text{ and } y_1 > y_2$$
$$\text{or}$$
$$x_1 > x_2 \text{ and } y_1 &lt; y_2$$
If $x_1 = x_2 \text{ or } y_1 = y_2$, the pair is neither concordant nor discordant.</p>

<p>Kendall&rsquo;s Tau is then defined as:
$$\tau = \frac{n_c-n_d}{\frac{1}{2} n (n-1) }$$
Where $n_c$ is the number of concordant pairs and $n_d$ is the number of discordant pairs.
Since $n (n-1) / 2$ is the total number of pairs, this value ranges from -1 to 1.</p>

<p>Unfortunately, this approach doesn&rsquo;t deal well with tied values.  Consider the following set of $(x,y)$ observations:
$$(1,1), (1,1), (2,2), (3,3)$$
There&rsquo;s a perfectly positive linear relationship between X and Y, but only 5 of the 6 pairs are concordant.  For this case we want to use the $\tau_B$ modified version:</p>

<p>$$\tau_B = \frac{n_c-n_d}{\sqrt{(n_0-n_1)(n_0-n_2)}}$$</p>

<p>&hellip;where:</p>

<p>$$n_0 = n(n-1)/2$$
$$n_1 = \text{Number of pairs with tied values in } X$$
$$n_2 = \text{Number of pairs with tied values in } Y$$</p>

<h2>Computing Naively</h2>

<p>We can compute $\tau_B$ in $O(n^{2})$ by looking at every pair of observations and tallying the number of concordant, discordant, and tied pairs.  Once we have the tallies, we&rsquo;ll apply the formula:
```clojure
(defn kendalls-tau-from-tallies
  [{:keys [concordant discordant pairs x-ties y-ties]}]
  (/ (&ndash; concordant discordant)</p>

<pre><code> (Math/sqrt (* (- pairs x-ties)
               (- pairs y-ties)))))
</code></pre>

<p>```</p>

<p>For a given pair of observations, we&rsquo;ll construct a map describing which tallies it will contribute to:
```clojure
(defn kendall-relations [[[x1 y1] [x2 y2]]]
  (cond
   (and (= x1 x2) (= y1 y2)) {:x-ties 1 :y-ties 1}
   (= x1 x2) {:x-ties 1}
   (= y1 y2) {:y-ties 1}
   (or (and (&lt; x1 x2) (&lt; y1 y2))</p>

<pre><code>   (and (&gt; x1 x2) (&gt; y1 y2))) {:concordant 1}
</code></pre>

<p>   :else {:discordant 1}))
```</p>

<p>Now we need a way of generating every pair:
```clojure
(defn pairs [[o &amp; more]]
  (if (nil? o) nil</p>

<pre><code>  (concat (map #(vector o %) more)
          (lazy-seq (pairs more)))))
</code></pre>

<p>;; (pairs [1 2 3 4])
;; => ([1 2] [1 3] [1 4] [2 3] [2 4] [3 4])
<code>
Finally, we put it all together by computing the relations tally for each pair and combining them using `merge-with`:
</code>clojure
(defn naive-kendalls-tau [xs ys]
  (let [observations (map vector xs ys)</p>

<pre><code>    relations (map kendall-relations (pairs observations))
    tallies (reduce (partial merge-with +
                             {:pairs 1})
                    {:concordant 0 :discordant 0
                     :x-ties 0 :y-ties 0 :pairs 0}
                    relations)]
(kendalls-tau-from-tallies tallies)))
</code></pre>

<p>```</p>

<h2>Knight&rsquo;s Algorithm</h2>

<p>In 1966, William R. Knight was a visiting statistician at the Fisheries Research Board of Canada.  He wrote:</p>

<blockquote><p>The problem of calculating Kendall&rsquo;s tau arose while attempting to evaluate species associations in catches by the Canadian east coast offshore fishery.  Sample sizes ranging up to 400 were common, making manual calculations out of the question; indeed, an initial program using an asymptotically inefficient method proved expensively slow.</p></blockquote>

<p>Necessity is the mother of invention, so he came up with a clever algorithm for computing Kendall&rsquo;s Tau in $O(n \log{n})$ which he published in his paper entitled &ldquo;<a href="http://www.jstor.org/stable/2282833">A Computer Method for Calculating Kendall&rsquo;s Tau with Ungrouped Data</a>&rdquo;.</p>

<p>First, sort the observations by their $x$ values using your favorite $O(n \log{n})$ algorithm.  Next, sort <em>that</em> sorted list by the $y$ values using a slightly modified <a href="http://en.wikipedia.org/wiki/Merge_sort">merge sort</a> that keeps track of the size of the swaps it had to perform.</p>

<p>Recall that merge sort works as follows:</p>

<ol>
<li>Divide the unsorted list into $n$ sublists, each containing 1 element (a list of 1 element is considered sorted).</li>
<li>Repeatedly merge sublists to produce new sublists until there is only 1 sublist remaining. This will be the sorted list.</li>
</ol>


<p><img class="center  <a" src="href="http://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif">http://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif</a>" title="Merge Sort Animation" >
<em>(description and animation from <a href="http://en.wikipedia.org/wiki/Merge_sort">Wikipedia</a>)</em></p>

<p>The trick is performed when merging sublists.  The list was originally sorted by $x$ values, so whenever an element from the second sublist is smaller than the next element from the first sublist we know that the corresponding observation is discordant with however many elements remain in the first sublist.</p>

<p>We can implement this modified merge sort by first handling the case of merging two sorted sequences:</p>

<p>```clojure
(defn merge-two-sorted-seqs-and-count-discords
  &ldquo;Takes a sequence containing two sorted sequences and merges them.  If an
element from the second sequence is less than the head of the first sequence, we
know that it was discordant with all the elements remaining in the first
sequence.  This is the insight that allows us to avoid the O(n<sup>2</sup>) comparisons in
the naive algorithm.</p>

<p>A tuple containing the count of discords and the merged sequence is returned.&ldquo;
  [[coll1 coll2]]
  (loop [swaps 0</p>

<pre><code>     ;; Explicitly track the remaining counts to avoid doing a linear
     ;; scan of the sequence each time, which would get us back to O(n^2)
     remaining-i (count coll1)
     remaining-j (count coll2)

     [i &amp; rest-i :as all-i] coll1
     [j &amp; rest-j :as all-j] coll2
     result []]
(cond
 (zero? remaining-j) [swaps (concat result all-i)]
 (zero? remaining-i) [swaps (concat result all-j)]
 (&lt;= i j) (recur swaps
                 (dec remaining-i) remaining-j
                 rest-i all-j (conj result i))
 :j&gt;i (recur (+ swaps remaining-i)
              remaining-i (dec remaining-j)
              all-i rest-j (conj result j)))))
</code></pre>

<p><code>
Now, we can do the full merge sort by applying that function to piece sizes that double until the whole collection is covered by a single sorted piece:
</code>clojure
(defn merge-sort-and-count-discords
  &ldquo;Returns a vector containing the number of discordant swaps and the sorted
collection.&rdquo;
  [coll]
  (loop [swaps 0</p>

<pre><code>     coll coll
     piece-size 1]
(let [pieces (partition-all piece-size coll)
      piece-pairs (partition-all 2 pieces)]
  (if (-&gt; piece-pairs first second)
    (let [[new-swaps new-coll]
          (-&gt;&gt; piece-pairs
               (map merge-two-sorted-seqs-and-count-discords)
               (reduce (fn [[acc-s acc-c] [s c]]
                         [(+ acc-s s) (concat acc-c c)])
                       [0 []]))]
      (recur (+ swaps new-swaps) new-coll (* 2 piece-size)))
    [swaps coll]))))
</code></pre>

<p>```</p>

<p>The only thing we are missing now is the tallies of tied pairs.  We could use <a href="http://clojuredocs.org/clojure_core/clojure.core/frequencies"><code>clojure.core/frequencies</code></a>, but Knight&rsquo;s original paper alludes to a different way which takes advantage of the fact that at different stages of the algorithm we have the list sorted by $X$ and then $Y$.  Most implementations do something like:</p>

<p>```clojure
(defn tied-pair-count [sorted-coll]
  (&ndash;>> sorted-coll</p>

<pre><code>   (partition-by identity)
   (map count)
   (map #(/ (* % (dec %)) 2))
   (reduce +)))
</code></pre>

<p>```</p>

<p>Now we have all the pieces, so we just have to put them together:</p>

<p>```clojure
(defn knights-kendalls-tau [xs ys]
  (let [observations (sort (map vector xs ys))</p>

<pre><code>    n (count observations)
    pair-count (/ (* n (dec n)) 2)
    xy-pair-ties (tied-pair-count observations)
    x-pair-ties (tied-pair-count (map first observations))
    [swaps sorted-ys] (merge-sort-and-count-discords
                       (map second observations))
    y-pair-ties (tied-pair-count sorted-ys)
    concordant-minus-discordant (- pair-count
                                   x-pair-ties
                                   y-pair-ties
                                   (- xy-pair-ties)
                                   (* 2 swaps))]
(/ concordant-minus-discordant
   (Math/sqrt (* (- pair-count x-pair-ties)
                 (- pair-count y-pair-ties))))))
</code></pre>

<p>```</p>

<h2>Conclusion</h2>

<p>There are certainly many things I would write differently above if I was really trying for performance.  The goal here was to clearly illustrate the algorithm and maintain the asymptotic run-time characteristics.</p>

<p>Also, I recently submitted <a href="https://issues.apache.org/jira/browse/MATH-814">a patch</a> to the Apache Commons Math library that contains an implementation of this in pure Java if that&rsquo;s your thing.</p>

<p>I think this algorithm is a clever little gem and I really enjoyed learning it.  Deconstructing a familiar algorithm like merge sort and utilizing its internal operations for some other purpose is a neat approach that I&rsquo;ll definitely keep in my algorithmic toolbox.</p>
]]></content>
  </entry>
  
</feed>
